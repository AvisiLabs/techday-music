{"cells":[{"cell_type":"markdown","metadata":{"id":"g6d501wgsL55"},"source":["# Classify Audio using CNN's\n","\n","Content:\n","1. Load GTZAN dataset\n","2. Explore the dataset\n","3. Train a default model \n","4. Alter the model to achieve better results\n","5. Train a model with your own music\n","6. Using your own model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PPbjSdM3Ga6"},"outputs":[],"source":["%pip install -q tensorflow===2.8.0 librosa===0.9.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pCPpF4Zy6Il"},"outputs":[],"source":["import glob\n","import os\n","import warnings\n","\n","from pathlib import Path\n","from random import randint\n","from typing import List, Optional, Tuple\n","\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import shutil\n","import tensorflow as tf\n","\n","from IPython.display import Audio, Image\n","from IPython.core.display import display\n","from google.colab import files\n","from sklearn.preprocessing import minmax_scale\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LeakyReLU, ReLU, Activation, BatchNormalization, Rescaling\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import RMSprop, Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import utils"]},{"cell_type":"markdown","metadata":{"id":"FXiRlN1j3ZPv"},"source":["# 1. Load GTZAN dataset"]},{"cell_type":"markdown","metadata":{"id":"96WkciYjKewE"},"source":["Obtain the dataset by running the following cell which clones the repository, containing the .wav-files and training data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H17Ed6heKf7e"},"outputs":[],"source":["!git clone --depth 1 https://github.com/AvisiLabs/techday-music.git\n","gtzan_path = \"/content/techday-music/gtzan-genres\"\n","gtzan_processed_path = \"/content/techday-music/gtzan-processed\""]},{"cell_type":"markdown","metadata":{"id":"425RjN8v0_OH"},"source":["# 2. Explore the dataset"]},{"cell_type":"markdown","metadata":{"id":"iVAKV2MmKkW0"},"source":["First we define some functions that will help us while exploring and preprocessing our data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWsql82gBiZh"},"outputs":[],"source":["HOP_LENGTH = 512\n","\n","def load_audio(path: str) -> Tuple[np.ndarray, int]:\n","  \"\"\"\n","    Loads audio using Librosa.\n","    In this way the audio is resampled to a sample rate of 22050Hz, \n","    the data is normalised to a range between -1 and 1\n","    and the audio channels are flattened into mono.\n","  \"\"\"\n","  return librosa.load(path)\n","\n","def show_audio_player(audio: np.ndarray, sample_rate: int) -> None:\n","  \"\"\"\n","    Show an audio player for the given audio and sample rate\n","  \"\"\"\n","  display(Audio(audio, rate=sample_rate))\n","\n","def show_waveform(audio: np.ndarray, sample_rate: int) -> None:\n","  \"\"\"\n","    Show a waveform for the given audio and sample rate\n","  \"\"\"\n","  plt.figure(figsize = (16,6))\n","  librosa.display.waveshow(y=audio, sr=sample_rate)\n","  plt.show()\n","  plt.close()\n","\n","def create_mel_spectrogram(audio: np.ndarray, sample_rate: int, file_name: Optional[str] = None) -> None:\n","  S = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128,\n","                                    fmax=8000)\n","  DB = librosa.amplitude_to_db(S, ref=np.max)\n","  fig, ax = plt.subplots(figsize=(16, 6))\n","\n","  if file_name != None:\n","    plt.axis('off')\n","    librosa.display.specshow(DB, sr=sample_rate, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel', cmap='gray')\n","    plt.savefig(file_name, bbox_inches='tight', pad_inches=0, transparent=False)\n","  else:\n","    librosa.display.specshow(DB, sr=sample_rate, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')\n","    plt.show()\n","\n","  plt.clf()\n","  plt.close()\n","\n","def inspect_audio_file(file_path: str) -> None:\n","  audio, sample_rate = load_audio(file_path)\n","  print(\"Audio player:\")\n","  show_audio_player(audio=audio, sample_rate=sample_rate)\n","  print(\"Waveform:\")\n","  show_waveform(audio=audio, sample_rate=sample_rate)\n","  print(\"Mel spectrogram:\")\n","  create_mel_spectrogram(audio, sample_rate)\n"]},{"cell_type":"markdown","metadata":{"id":"TS1v4k7-OJSL"},"source":["After that, we are able to load some example files.\n"]},{"cell_type":"code","source":["inspect_audio_file(os.path.join(gtzan_path, \"pop/pop.00051.wav\"))"],"metadata":{"id":"Gi8sQH7jp60j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuLpazkaAmJK"},"outputs":[],"source":["inspect_audio_file(os.path.join(gtzan_path, \"pop/pop.00038.wav\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6uaM2trPQOG"},"outputs":[],"source":["inspect_audio_file(os.path.join(gtzan_path, \"metal/metal.00010.wav\"))"]},{"cell_type":"markdown","metadata":{"id":"x6tXRunIzeUx"},"source":["To see that the spectrograms for different genres do differ, we are displaying 5 examples for both metal and classical songs.\n","\n","Note that the images are grayscale from now on. This is for performance improvements while training the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNFQ9YLAQXiy"},"outputs":[],"source":["def display_examples_for_genre(genre: str):\n","  directory = os.path.join(gtzan_processed_path, genre)\n","  \n","  for i in range(10,15):\n","    filename = os.path.join(directory, genre + \".000\" + str(i) + \"-0.png\")\n","    print(filename)\n","    display(Image(filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIyfdMMCSBw9"},"outputs":[],"source":["display_examples_for_genre(\"metal\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-V2lKzuoSBmK"},"outputs":[],"source":["display_examples_for_genre(\"classical\")"]},{"cell_type":"markdown","metadata":{"id":"7UbI0Okqz0_9"},"source":["# 3. Train a default model"]},{"cell_type":"markdown","metadata":{"id":"Ybl95iJuLDqt"},"source":["First we change the runtime type of our Colab Notebook to a GPU one: Runtime -> Change Runtime Type -> Choose for GPU under \"Hardware Accelerator\". This is needed to ensure we have enough performance for training the model quickly.\n","\n","Because this connects us to a new runtime, we have to rerun the previous cells. For this press the shortcut CMD/ctrl + F8 / Runtime -> Run before. Make sure you do this after selecting this cell. This can take a while, so this might be a good time to grab a new beer üç∫"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBJqURwFLTgl","cellView":"form"},"outputs":[],"source":["#@title Genres\n","#@markdown Choose three distinct genres that will be used for training and inference.\n","#@markdown Make sure you run this cell after selecting your genres.\n","first_genre = 'jazz' #@param [\"blues\",\"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n","second_genre = 'hiphop' #@param [\"blues\",\"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n","third_genre = 'pop' #@param [\"blues\",\"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n","\n","genres_to_copy = [first_genre, second_genre, third_genre]\n","genres_to_copy.sort()"]},{"cell_type":"markdown","metadata":{"id":"KWT5NVpQOOD8"},"source":["# Copy dataset\n","We copy the folders of your selected genres to our dataset folder.\n","\n","The dataset is structured as follows:\n","\n","- dataset\n","  - blues\n","    - 00000.png\n","    - 00001.png\n","    - ...\n","  - classical\n","    - 00000.png\n","    - 00001.png\n","    - ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyt67E-wOMpm"},"outputs":[],"source":["dataset_folder = \"/content/dataset\"\n","!mkdir -p {dataset_folder}\n","dataset_path = os.path.join(dataset_folder, \"gtzan_processed\")\n","if not os.path.exists(dataset_path):\n","  for genre in genres_to_copy:\n","    genre_git_path = os.path.join(gtzan_processed_path, genre)\n","    genre_path = os.path.join(dataset_path, genre)\n","    shutil.copytree(genre_git_path, genre_path)"]},{"cell_type":"markdown","metadata":{"id":"uQOb5IVNOq7P"},"source":["# Create dataset from files\n","Since our images are already grouped by genre, we can use the `image_dataset_from_directory` function to load everything into a dataset. \n","\n","We split the dataset into two parts: one dataset for training the model and a smaller dataset (30% of all data) for evaluating how our model performs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZLginx2OvI5"},"outputs":[],"source":["INPUT_SHAPE = (int(326 / 2), int(892 / 2), 1)\n","COLOR_MODE = \"grayscale\"\n","VALIDATION_SPLIT = 0.3\n","SEED = 123\n","BATCH_SIZE = 16\n","\n","train_dataset = utils.image_dataset_from_directory(\n","    dataset_path,\n","    color_mode=COLOR_MODE,\n","    validation_split=VALIDATION_SPLIT,\n","    subset=\"training\",\n","    seed=SEED,\n","    image_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n","    batch_size=BATCH_SIZE\n",").cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","validation_dataset = utils.image_dataset_from_directory(\n","    dataset_path,\n","    color_mode=COLOR_MODE,\n","    validation_split=VALIDATION_SPLIT,\n","    subset=\"validation\",\n","    seed=SEED,\n","    image_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n","    batch_size=BATCH_SIZE\n",").cache().prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"Wx1-vTToO-rE"},"source":["# Define model\n","\n","We define our model, which consists of several blocks:\n","* Rescaling, which normalizes our image data by converting 8-bit color to a floating point number\n","* Convolution, which has kernels that move over the image to obtain information about the image (features)\n","* Pooling, which reduce the number of parameters by combining information from previous layers\n","* Output, which connects the features from the final layers to the labels we can assign to the input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SAdhzguPHzc"},"outputs":[],"source":["def define_model(number_of_classes: int):\n","    result_model = Sequential(name='SoundClassifier')\n","    \n","    base_kernels = 8\n","    \n","    weight_decay = 1e-4\n","\n","    result_model.add(Rescaling(1./255, input_shape=INPUT_SHAPE))\n","    \n","    # CONV1\n","    result_model.add(Conv2D(base_kernels, kernel_size=5, strides=1, padding='same', kernel_regularizer=l2(weight_decay)))\n","    result_model.add(ReLU())\n","    result_model.add(BatchNormalization())\n","    \n","    # CONV2\n","    result_model.add(Conv2D(base_kernels, kernel_size=5, strides=1, padding='same', kernel_regularizer=l2(weight_decay)))\n","    result_model.add(ReLU())\n","    result_model.add(BatchNormalization())\n","    \n","    # POOL + dropout\n","    result_model.add(MaxPooling2D(pool_size=3, strides=3))\n","    result_model.add(Dropout(rate=0.4))\n","    \n","    # CONV3\n","    result_model.add(Conv2D(base_kernels * 2, kernel_size=3, strides=3, padding='same', kernel_regularizer=l2(weight_decay)))\n","    result_model.add(ReLU())\n","    result_model.add(BatchNormalization())\n","    \n","    # POOL + dropout\n","    result_model.add(MaxPooling2D(pool_size=3, strides=3))\n","    result_model.add(Dropout(rate=0.4))\n","    \n","    # FC layers\n","    result_model.add(Flatten())\n","    result_model.add(Dense(number_of_classes, activation = 'softmax'))\n","    result_model.summary()\n","    return result_model\n","\n","model = define_model(len(genres_to_copy))\n","\n","model.compile(\n","    loss='sparse_categorical_crossentropy', \n","    optimizer=Adam(learning_rate=0.01, decay=1e-6), \n","    metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"DANRcla4PKdR"},"source":["# Train model\n","\n","We train the defined model using the dataset which we have loaded. The training runs for 30 epochs, which means the dataset is presented 30 times to the model. We also shuffle the data every epoch. By doing so, the model can learn from all samples regardless of the order in which it is presented to the model. \n","\n","During training you will see various metrics in the log-output. The loss metric represents how far the model is from giving a perfect prediction. The accuracy metric tells you how many labels were correctly predicted. \n","\n","The loss and accuracy metric are for the training dataset (from which the model is learning) and the val_loss and val_accuracy metrics are for the validation dataset. Since the validation dataset contains samples which the model has not seen yet, this will be the most important metric to watch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaywXnEzPPvh"},"outputs":[],"source":["checkpoint_folder = \"/content/soundclassifier_checkpoint\"\n","!mkdir -p {checkpoint_folder}\n","checkpoint_filename = \"model.weights.best.hdf5\"\n","checkpoint_path = os.path.join(checkpoint_folder, checkpoint_filename)\n","\n","def train_model(model):\n","    checkpointer = ModelCheckpoint(filepath=checkpoint_path, verbose=1, \n","                               save_best_only=True)\n","\n","    hist = model.fit(\n","        train_dataset, \n","        validation_data=validation_dataset, \n","        batch_size=BATCH_SIZE, \n","        epochs=30,\n","        callbacks=[checkpointer], \n","        verbose=1)\n","    return hist\n","\n","history = train_model(model)\n","def print_training_history(history):\n","    plt.plot(history.history['accuracy'], label='train accuracy')\n","    plt.plot(history.history['val_accuracy'], label='validation accuracy')\n","    plt.legend()\n","    plt.show()\n","print_training_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZ55EJchPeG7"},"outputs":[],"source":["def get_prediction(img):\n","    arr = np.array(img)\n","    arr = arr.reshape(INPUT_SHAPE)\n","    arr = np.expand_dims(arr, axis=0)\n","    prediction = model.predict(arr)\n","    \n","    bestclass = ''\n","    bestconf = -1\n","    for n in [0,1,2]:\n","        if prediction[0][n] > bestconf:\n","            bestclass = str(n)\n","            bestconf = prediction[0][n]\n","    return (bestclass, bestconf)\n","\n","def visualize_prediction(image, sorted_labels):\n","    plt.figure(figsize=(16, 8), dpi=80)\n","    plt.axis('off')\n","    plt.imshow(image.numpy().astype(\"uint8\").squeeze(axis=2), cmap=\"gray\")\n","    (bestclass, bestconf) = get_prediction(image)\n","    print(f\"Predicted genre: {sorted_labels[int(bestclass)]}, confidence: {bestconf}\")\n","    plt.show()\n","    plt.close()\n","\n","def visualize_random_prediction(dataset):\n","    for images, labels in dataset.shuffle(1000).take(1):\n","      i = randint(0, len(images)-1)\n","      label = labels[i]\n","      image = images[i]\n","      print(f\"Actual genre: {genres_to_copy[label.numpy()]}\")\n","      visualize_prediction(image, genres_to_copy)\n","\n","visualize_random_prediction(validation_dataset)"]},{"cell_type":"markdown","source":["Want to see if the model also performs on your own music? Run the following cells to upload a file and pass it to the trained model."],"metadata":{"id":"XBY6nbg4zYP5"}},{"cell_type":"code","source":["def read_image(file_path):\n","  img = tf.io.read_file(file_path)\n","  img = tf.io.decode_png(img, channels=1)\n","  return tf.image.resize(img, [INPUT_SHAPE[0], INPUT_SHAPE[1]])\n","\n","def predict_for_uploaded(uploaded, labels):\n","  upload_base_path = \"/content\"\n","  uploaded_file_name = next(iter(uploaded))\n","  uploaded_file_path = os.path.join(upload_base_path, uploaded_file_name)\n","\n","  inference_audio, inference_sample_rate = load_audio(uploaded_file_path)\n","\n","  inference_samples = take_samples_from_audio(inference_audio, inference_sample_rate)\n","\n","  for i, sample in enumerate(inference_samples):\n","    show_audio_player(sample, inference_sample_rate)\n","    processed_file_path = os.path.join(upload_base_path, os.path.splitext(uploaded_file_name)[0] + \"-\" + str(i) + \".png\")\n","    create_mel_spectrogram(audio=sample, sample_rate=inference_sample_rate, file_name=processed_file_path)\n","    image = read_image(processed_file_path)\n","    visualize_prediction(image, labels)\n","    print()\n","\n","def take_samples_from_audio(audio: np.ndarray, sample_rate: int) -> List[np.ndarray]:\n","  samples = []\n","  total_length = len(audio)\n","  mid = int(total_length / 2)\n","\n","  if (total_length == 30 * sample_rate):\n","    samples.append(audio)\n","  if (total_length >= 60 * sample_rate):\n","    samples.append(audio[mid - (sample_rate * 30): mid])\n","    samples.append(audio[mid: mid + (sample_rate * 30)])\n","  elif (total_length < 60  * sample_rate and total_length >= 30 * sample_rate):\n","    samples.append(audio[mid - (sample_rate * 15): mid + (sample_rate * 15)])\n","  else:\n","    print(\"Audio must be equal to or longer than 30 seconds.\")\n","\n","  return samples"],"metadata":{"id":"Q8H7gxkdzUqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uploaded = files.upload()\n","predict_for_uploaded(uploaded, genres_to_copy)"],"metadata":{"id":"5z8M18aszmJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BcihzfpBz9Rp"},"source":["# 4. Alter the model to achieve better results\n","\n","The model isn't that good at predicting the correct genre (yet!).\n","Change the hyperparameters of the model above, then retrain the model by running the cells again.\n","\n","Think of:\n","- Number of epochs\n","- Number of layers\n","- Number of kernels, kernel size or stride\n","- Pooling type and size\n","- Optimizer and learning rate\n"]},{"cell_type":"markdown","metadata":{"id":"8jrNDisVKV0m"},"source":["# 5. Train a model with your own music"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HfrzKv0PnJME","cellView":"form"},"outputs":[],"source":["#@markdown Choose if you want the data to be read from and persisted to a Google Drive folder (which is created for you). \n","#@markdown In this way, you don't have to process your music files every time you run this notebook.\n","#@markdown Check or uncheck the checkbox and run this cell.\n","persist_processed_data_on_drive = True #@param {type:\"boolean\"}\n","\n","music_path = None\n","music_processed_path = None\n","if persist_processed_data_on_drive:\n","  print(\"Data will be persisted on Drive\")\n","  from google.colab import drive\n","  mount_path = \"/content/drive\"\n","  drive.mount(mount_path, force_remount=True)\n","  music_path = os.path.join(mount_path, \"MyDrive/music\")\n","  music_processed_path =  os.path.join(mount_path, \"MyDrive/music-processed\")\n","else:\n","  print(\"Data will NOT be persisted on Drive\")\n","  music_path = \"/content/music\"\n","  music_processed_path = \"/content/music-processed\"\n","\n","!mkdir -p {music_path} {music_processed_path}"]},{"cell_type":"markdown","metadata":{"id":"RleXq8F10n8H"},"source":["Earlier we used the GTZAN dataset which uses samples of 30 seconds long. For using our own music, we also want to take samples of 30 seconds."]},{"cell_type":"markdown","metadata":{"id":"IYFds_5__OXP"},"source":["The code beneath searches for all files inside the music path, takes samples from the files and create spectrograms for it. These spectrograms are written to the music-processed path.\n","\n","Depending on your choice for persisting data in Google Drive you can either upload your music files to the created Google Drive directory (with the name \"music\") or the music directory which can be found in the file explorer.\n","\n","The expected structure for the files in the music path is:\n","- One directory per different genre\n","- Inside a genre directory, file names must be unique\n","\n","The supported audio formats are:\n","- .aifc\n","- .aiff / .aif\n","- .au\n","- .flac\n","- .ogg\n","- .opus\n","- .wav\n","- .mp3\n","- .m4a / .mp4\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3ZfQEtBauj6"},"outputs":[],"source":["# Filter following warning: UserWarning: PySoundFile failed. Trying audioread instead.\n","warnings.filterwarnings('ignore')\n","\n","def spectrograms_exists_for_file(file: str):\n","  spectrograms_search_path = os.path.splitext(file.replace(music_path, music_processed_path))[0] + \"*.png\"\n","  return len(glob.glob(spectrograms_search_path)) != 0\n","\n","def create_spectrogram_for_file(file: str):\n","  if spectrograms_exists_for_file(file):\n","    print(\"Skipping because spectrograms already exist for\", file)\n","    return\n","\n","  print(\"Processing\", file)\n","\n","  audio, sample_rate = load_audio(file)\n","  samples = take_samples_from_audio(audio, sample_rate)\n","\n","  for i, sample in enumerate(samples):\n","    processed_file_path = Path(os.path.splitext(file.replace(music_path, music_processed_path))[0] + \"-\" + str(i) + \".png\")\n","    os.makedirs(processed_file_path.parent, exist_ok=True)\n","    create_mel_spectrogram(audio=sample, sample_rate=sample_rate, file_name=processed_file_path)\n"," \n","\n","print(os.path.join(music_path, '*/*'))\n","music_files = glob.glob(os.path.join(music_path, '*/*'))\n","\n","for file in music_files:  \n","  create_spectrogram_for_file(file)\n","\n","print(\"Finished creating spectrograms.\")\n","\n","custom_labels = next(os.walk(music_processed_path))[1]\n","custom_labels.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LulBIzs9VZrS"},"outputs":[],"source":["VALIDATION_SPLIT = 0.3\n","SEED = 420\n","BATCH_SIZE = 16\n","\n","custom_train_dataset = utils.image_dataset_from_directory(\n","    music_processed_path,\n","    color_mode=COLOR_MODE,\n","    validation_split=VALIDATION_SPLIT,\n","    subset=\"training\",\n","    seed=SEED,\n","    image_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n","    batch_size=BATCH_SIZE\n",").cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","custom_validation_dataset = utils.image_dataset_from_directory(\n","    music_processed_path,\n","    color_mode=COLOR_MODE,\n","    validation_split=VALIDATION_SPLIT,\n","    subset=\"validation\",\n","    seed=SEED,\n","    image_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n","    batch_size=BATCH_SIZE\n",").cache().prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","source":["We define a new model, which you can tweak again so it performs well for your own dataset."],"metadata":{"id":"CjzIY0b6kl5u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MufJZ3-LY_0Z"},"outputs":[],"source":["def define_model(number_of_classes: int):\n","    result_model = Sequential(name='SoundClassifier')\n","    \n","    base_kernels = 8\n","    \n","    weight_decay = 1e-4\n","\n","    result_model.add(Rescaling(1./255, input_shape=INPUT_SHAPE))\n","    \n","    # CONV1\n","    result_model.add(Conv2D(base_kernels, kernel_size=5, strides=1, padding='same', kernel_regularizer=l2(weight_decay)))\n","    result_model.add(ReLU())\n","    result_model.add(BatchNormalization())\n","    \n","    # CONV2\n","    result_model.add(Conv2D(base_kernels, kernel_size=5, strides=1, padding='same', kernel_regularizer=l2(weight_decay)))\n","    result_model.add(ReLU())\n","    result_model.add(BatchNormalization())\n","    \n","    # POOL + dropout\n","    result_model.add(MaxPooling2D(pool_size=3, strides=3))\n","    result_model.add(Dropout(rate=0.4))\n","    \n","    # CONV3\n","    result_model.add(Conv2D(base_kernels * 2, kernel_size=3, strides=3, padding='same', kernel_regularizer=l2(weight_decay)))\n","    result_model.add(ReLU())\n","    result_model.add(BatchNormalization())\n","    \n","    # POOL + dropout\n","    result_model.add(MaxPooling2D(pool_size=3, strides=3))\n","    result_model.add(Dropout(rate=0.4))\n","    \n","    # FC layers\n","    result_model.add(Flatten())\n","    result_model.add(Dense(number_of_classes, activation = 'softmax'))\n","    result_model.summary()\n","    return result_model\n","\n","model = define_model(len(next(os.walk(music_processed_path))[1]))\n","\n","model.compile(\n","    loss='sparse_categorical_crossentropy', \n","    optimizer=Adam(learning_rate=0.01, decay=1e-6), \n","    metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRbkNXMXZErW"},"outputs":[],"source":["custom_checkpoint_folder = \"/content/soundclassifier_custom_checkpoint\"\n","!mkdir -p {custom_checkpoint_folder}\n","custom_checkpoint_path = os.path.join(custom_checkpoint_folder, checkpoint_filename)\n","\n","def train_model(model):\n","    checkpointer = ModelCheckpoint(filepath=custom_checkpoint_path, verbose=1, \n","                               save_best_only=True)\n","\n","    hist = model.fit(\n","        custom_train_dataset, \n","        validation_data=custom_validation_dataset, \n","        batch_size=BATCH_SIZE, \n","        epochs=30,\n","        callbacks=[checkpointer], \n","        verbose=1)\n","    return hist\n","\n","history = train_model(model)\n","print_training_history(history)"]},{"cell_type":"markdown","source":["# 6. Using your own model\n","\n","Now that we've trained a model for your own data, we can use the model to classify new samples. You can upload a music file using the upload form below, after which the file is classified using the trained model."],"metadata":{"id":"AJc77cQirVjS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFFH5VWEcjcI"},"outputs":[],"source":["uploaded = files.upload()\n","predict_for_uploaded(uploaded, custom_labels)"]},{"cell_type":"code","source":[""],"metadata":{"id":"3hDV7FqsFI9L"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Techday.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}